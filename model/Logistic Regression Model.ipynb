{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Import titles using Pickle</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pickle\n",
    "\n",
    "def ReadFile(file_name, title_list):\n",
    "    \n",
    "    # Open the files ('rb' is for read binary)\n",
    "    file_object = open(file_name,'rb')\n",
    "    \n",
    "    # Load files into list using pickle\n",
    "    in_list = pickle.load(file_object)\n",
    "    \n",
    "    # Close files\n",
    "    file_object.close()\n",
    "    \n",
    "    # Add list just read in to existing list\n",
    "    title_list.extend(in_list)\n",
    "    \n",
    "    return(title_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96205\n",
      "105703\n"
     ]
    }
   ],
   "source": [
    "# Crate empty list for left sites\n",
    "left_title_list = []\n",
    "\n",
    "# Read left sites\n",
    "left_title_list = ReadFile(\"data/atlantic_titles\", left_title_list)\n",
    "left_title_list = ReadFile(\"data/mjones_titles\", left_title_list)\n",
    "left_title_list = ReadFile(\"data/newrepublic_titles\", left_title_list)\n",
    "left_title_list = ReadFile(\"data/nytimes_titles\", left_title_list)\n",
    "left_title_list = ReadFile(\"data/politico_titles\", left_title_list)\n",
    "left_title_list = ReadFile(\"data/slate_titles\", left_title_list)\n",
    "left_title_list = ReadFile(\"data/thedailybeast_titles\", left_title_list)\n",
    "left_title_list = ReadFile(\"data/theguardian_titles\", left_title_list)\n",
    "left_title_list = ReadFile(\"data/theintercept_titles\", left_title_list)\n",
    "left_title_list = ReadFile(\"data/washpost_titles\", left_title_list)\n",
    "\n",
    "#***MAKE SURE THIS NUMBER MATCHES THE NUMBER OF DOCUMENTS IN MONGODB***\n",
    "print(len(left_title_list))\n",
    "\n",
    "# Crate empty list for right sites\n",
    "right_title_list = []\n",
    "\n",
    "# Read right sites\n",
    "right_title_list = ReadFile(\"data/americanconservative_titles\", right_title_list)\n",
    "right_title_list = ReadFile(\"data/breitbart_titles\", right_title_list)\n",
    "right_title_list = ReadFile(\"data/dailywire_titles\", right_title_list)\n",
    "right_title_list = ReadFile(\"data/economist_titles\", right_title_list)\n",
    "right_title_list = ReadFile(\"data/fiscaltimes_titles\", right_title_list)\n",
    "right_title_list = ReadFile(\"data/foxnews_titles\", right_title_list)\n",
    "right_title_list = ReadFile(\"data/nypost_titles\", right_title_list)\n",
    "right_title_list = ReadFile(\"data/reason_titles\", right_title_list)\n",
    "right_title_list = ReadFile(\"data/thehill_titles\", right_title_list)\n",
    "right_title_list = ReadFile(\"data/washtimes_titles\", right_title_list)\n",
    "\n",
    "#***MAKE SURE THIS NUMBER MATCHES THE NUMBER OF DOCUMENTS IN MONGODB***\n",
    "print(len(right_title_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Add Site Bias to Dataframes</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   bias                                              title\n",
      "0     1  Conservative High Schoolers Want to ‘Own the L...\n",
      "1     1  The Instagram Forums Where Teens Go to Debate ...\n",
      "2     1  The Doomed Republican Attempt to Impeach Rod R...\n",
      "3     1                          Facebook Is Probably Fine\n",
      "4     1                  Secretary of a State of Confusion\n",
      "(96205, 2)\n",
      "\n",
      "   bias                                        title\n",
      "0     0               Fruits Of The Quiet Revolution\n",
      "1     0               Trust And Mistrust In Churches\n",
      "2     0   A Democratic President From Trump Country?\n",
      "3     0  ‘Arab NATO’: A Terrible Idea That Won’t Die\n",
      "4     0                       TAC Fall Intern Wanted\n",
      "(105703, 2)\n"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Bias of 1 -> Left; Bias of 0 -> Right\n",
    "df1 = pd.DataFrame({'title': np.array(left_title_list), 'bias': 1})\n",
    "df2 = pd.DataFrame({'title': np.array(right_title_list), 'bias': 0})\n",
    "\n",
    "print(df1.head())\n",
    "print(df1.shape)\n",
    "print()\n",
    "print(df2.head())\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Downsample Larger Dataframe</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96205, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "df2_downsampled = resample(df2, replace = False, n_samples = len(df1), random_state = 41)\n",
    "print(df2_downsampled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Combine Left & Right Dataframes</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(192410, 2)\n",
      "(192328, 2)\n"
     ]
    }
   ],
   "source": [
    "df_combined = pd.concat([df1, df2_downsampled])\n",
    "print()\n",
    "print(df_combined.shape)\n",
    "\n",
    "df_clean = df_combined.dropna()\n",
    "print(df_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Logistic Regression on Words in Title</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.77394867102\n"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a matrix of word count using CountVectorizer\n",
    "# Count single words and word pairs (ngram_range = 1-2)\n",
    "vectorizer = CountVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "\n",
    "# Fit and transform data\n",
    "X = vectorizer.fit_transform(df_clean['title'])\n",
    "\n",
    "# Create training and test split\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X, df_clean['bias'], random_state=41)\n",
    "\n",
    "# Create the model and fit the training data\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "# Show model's score\n",
    "print(logreg.score(X_test,y_test))\n",
    "\n",
    "coef_zip = zip(logreg.coef_.tolist()[0],list(vectorizer.get_feature_names()))\n",
    "coef_list = list(coef_zip)\n",
    "coef_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1769280\n"
     ]
    }
   ],
   "source": [
    "print(len(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.4998531,  0.5001469]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.predict_proba(vectorizer.transform(['healthcare']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Output Model Results to JSON File</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create empty list of dictionaries for titles\n",
    "title_dict = {}\n",
    "\n",
    "# Loop through title list and add to dictionary\n",
    "for i in range(0,len(coef_list)-1):\n",
    "    for item in coef_list[i]:\n",
    "        if type(item) is str:\n",
    "            title_dict[item] = logreg.predict_proba(vectorizer.transform([item]))[0][0]\n",
    "            \n",
    "import json\n",
    "\n",
    "# Write list of dictionaries to JSON file\n",
    "with open ('model-data-new.json', 'w') as outfile:\n",
    "    json.dump([title_list], outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create empty list of dictionaries for titles\n",
    "d = {}\n",
    "\n",
    "# Loop through title list\n",
    "for i in range(0,len(coef_list)-1):\n",
    "    for item in coef_list[i]:\n",
    "        if type(item) is str:\n",
    "            d[item] = logreg.predict_proba(vectorizer.transform([item]))[0][0]\n",
    "            \n",
    "sorted_dict = sorted(d.items(), key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted_dict[10000:20000])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
