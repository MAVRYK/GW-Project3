{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addToDB(DB_NAME,COL_NAME,PATH,FILE):\n",
    "    '''\n",
    "    Imports a file into mongoDB\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    DB_NAME : Name of the database to connect to\n",
    "    COL_NAME: Name of the collection to create\n",
    "    PATH    : Path to folder with the file\n",
    "    FILE  : Filename\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Collection COL_NAME in DB_NAME database\n",
    "    '''\n",
    "    !mongoimport --db {DB_NAME} --collection {COL_NAME} --file {PATH+FILE} --batchSize 1\n",
    "    print(f'Collection {COL_NAME} in {DB_NAME} database created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor(html):\n",
    "    '''\n",
    "    Parse html using newspaper\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    html   : 'string'\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    result : dictionary\n",
    "    \n",
    "    '''\n",
    "    import newspaper\n",
    "    from bs4 import BeautifulSoup as bs\n",
    "    from datetime import datetime\n",
    "    \n",
    "    article = newspaper.Article('')\n",
    "    article.set_html(html)\n",
    "    \n",
    "    try:\n",
    "        article.build()\n",
    "    except Exception as e:\n",
    "        print(f'feature_extractor: None features found. Exception: {e}')\n",
    "        return {'text':''}\n",
    "        \n",
    "    \n",
    "    # parse date manually if it wasn't found by newspaper\n",
    "    if not article.publish_date:\n",
    "        sp = bs(html, 'lxml')\n",
    "        # 'dek___3AQpw' class appears on 30% of msnbc websites\n",
    "        try:\n",
    "            # msnbc.com date\n",
    "            publish_date = datetime.strptime(\\\n",
    "                               (sp.find('p', class_='dek___3AQpw').span.text),\\\n",
    "                               '%b.%d.%Y'\\\n",
    "                            )\n",
    "        except:\n",
    "            publish_date = ''\n",
    "    else:\n",
    "        publish_date = article.publish_date\n",
    "\n",
    "\n",
    "    return {\n",
    "            'date'    :publish_date,\n",
    "            'title'   :article.title,\n",
    "            'text'    :article.text,\n",
    "            'authors' :article.authors,\n",
    "            'keywords':article.keywords\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs_parser(htmlCol, skip=True):\n",
    "    '''\n",
    "    Parse mongo docs, extract features and update the doc with the features\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    htmlCol : mongodb collection, has to have documents with 'html' key\n",
    "    skip    : skip html processing if meta key exists in a record, default \"True\"\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    updates all documents in the collection\n",
    "    '''\n",
    "    \n",
    "    try: \n",
    "        for doc in htmlCol.find():\n",
    "            print(f\"{htmlCol.name}:{doc['_id']}:\")\n",
    "\n",
    "            if 'html' in doc:\n",
    "                if 'meta' in doc and skip: \n",
    "                    print('Meta exists, skipping')\n",
    "                else:\n",
    "                    # extract metadata from html\n",
    "                    meta = feature_extractor(doc['html'])\n",
    "\n",
    "                    try:\n",
    "                        if meta == doc['meta']:\n",
    "                            print(f\"has same meta\")\n",
    "                    except:\n",
    "                        # if there is newer meta data or meta key is not existing\n",
    "                        htmlCol.update_one(\n",
    "                            {'_id':ObjectId(doc['_id'])},\n",
    "                            {'$set' : {\n",
    "                                      'meta' : meta\n",
    "                                      }\n",
    "                            }\n",
    "                        )\n",
    "                        print(f\"saved meta\")\n",
    "            else:\n",
    "                print(f\"does not have html\")\n",
    "            print('----------')\n",
    "    except:\n",
    "        print(f\"docs_parser: couldn't find docs in collection {htmlCol.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_doc(db, collection, id):\n",
    "    '''\n",
    "    Finds a document by 'id' and prints contents to the console\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    db         : database name\n",
    "    collection : mongodb collection\n",
    "    id         : mongodb document id\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Prints first 100 symbols of each document's key to console\n",
    "    '''\n",
    "    from bson.objectid import ObjectId\n",
    "    doc = db['collection'].find_one({'_id':ObjectId(id)})\n",
    "    for k in doc:\n",
    "        print(f\"{k} : {str(doc[k])[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some comands to keep dbs clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deletes all 'meta' fields from all docs\n",
    "# htmlCol.update({}, {$unset: {meta:1}}, false, true); # mongo shell comand\n",
    "htmlCol.update({}, {'$unset': {'meta':1}}, multi=True) # pymongo way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leaves only unique documents by 'url' field\n",
    "\n",
    "htmlCol.create_index(\n",
    "    \"url\",\n",
    "    unique=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pymongo 'find' returns cursor that allows iterating through results\n",
    "# calling first object [0] allows accessing the dictionary with results\n",
    "# the ['html'] is the key in the dictionary\n",
    "html = htmlCol.find({'url':'http://www.msnbc.com/velshi-ruhle/watch/jeff-sessions-is-justifying-harsh-immigration-policy-with-the-bible-1256689731629'},\\\n",
    "            projection={'html':True, '_id':False})[0]['html']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find documents NOT containing a 'tag': regex expression\n",
    "import re\n",
    "tag = re.compile('dek___3AQpw.')\n",
    "docs = htmlCol.find({\"html\" : {'$not': tag}})\n",
    "for d in docs[:20]: print(d['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "\n",
    "# use multiprocessing to extract features\n",
    "def func():\n",
    "    DB_NAME = 'scrape'\n",
    "    db = pm.MongoClient(host='localhost', port=27017, maxPoolSize=500)[DB_NAME]\n",
    "\n",
    "    for collection in ['left','right']: docs_parser(db[collection])\n",
    "\n",
    "proc = Process(target=func)\n",
    "proc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Production code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging turn on logging to console\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "# dependencies\n",
    "import pymongo as pm\n",
    "\n",
    "from resources.newspaper import newspaper\n",
    "from resources.newspaper.newspaper.source import Source\n",
    "# dir(Source)\n",
    "\n",
    "# Initialize PyMongo to work with MongoDBs\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pm.MongoClient(conn, maxPoolSize=200)\n",
    "\n",
    "# define db \n",
    "# DB_NAME = 'scrape'\n",
    "DB_NAME = 'FINALP'\n",
    "db = client[DB_NAME]\n",
    "\n",
    "SOURCES = {\n",
    "    'left'  : [\n",
    "        'https://newrepublic.com',\n",
    "        'https://www.motherjones.com'\n",
    "# 3. Slate\n",
    "# 4. The Intercept\n",
    "# 5. Daily Beast\n",
    "# 6. The Atlantic\n",
    "# 7. Washington Post\n",
    "# 8. Politico\n",
    "# 9. The Guardian\n",
    "# 10. BBC\n",
    "    ],\n",
    "    'right' : [\n",
    "        'https://www.breitbart.com'\n",
    "# 2. Fox News\n",
    "# 3. New York Post\n",
    "# 4. The American Conservative\n",
    "# 5. Washington Times\n",
    "# 6. Daily Wire\n",
    "# 7. The Fiscal Times\n",
    "# 8. The Hill\n",
    "# 9. The Daily Caller\n",
    "# 10. Reason\n",
    "    ]\n",
    "}\n",
    "\n",
    "def saveToDB(db, collection, url, html, meta={}):\n",
    "    \"\"\"\n",
    "    Saves a document to mongoDB, making sure there are no duplicates by \n",
    "    'url' value\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    db, collection  : mongo db connection\n",
    "    url, html, meta : values to store\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Saved document\n",
    "    \"\"\"\n",
    "    collection = db[collection]\n",
    "    collection.update_one(\n",
    "        {'url' : url},\n",
    "        {\n",
    "            '$set':\n",
    "                {'url' : url,\n",
    "                 'html' : html,\n",
    "                 'meta' : meta\n",
    "                }\n",
    "        }\n",
    "        ,\n",
    "        upsert=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(url, db, collection, latest_date=None):\n",
    "    '''\n",
    "    Scrapes all articles from a 'url' up to the 'latest_date'\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    url         : main news website url\n",
    "    latest_date : YYYY-MM-DD\n",
    "    db          : database name\n",
    "    collection  : mongodb collection\n",
    "    verbose     : turn loggint to stdout\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Articles' urls and html to media\n",
    "    \n",
    "    '''\n",
    "\n",
    "    source = Source(url)\n",
    "    source.download()\n",
    "    source.parse()\n",
    "    \n",
    "    if source.html:\n",
    "        saveToDB(\n",
    "            db=db, collection=collection,\n",
    "            url=source.url, \n",
    "            html=source.html\n",
    "        )\n",
    "\n",
    "    logging.debug(f'{source.url} parsed')\n",
    "    \n",
    "    source.set_categories()\n",
    "    source.download_categories()\n",
    "    source.parse_categories()\n",
    "    \n",
    "    return source\n",
    "    \n",
    "#     new_rep.download_categories()\n",
    "#     new_rep.parse_categories()\n",
    "#     new_rep.set_feeds()\n",
    "#     new_rep.download_feeds()\n",
    "#     new_rep.generate_articles()\n",
    "#     new_rep.download_articles()\n",
    "#     new_rep.parse_articles()\n",
    "#     new_rep.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): newrepublic.com\n",
      "DEBUG:urllib3.connectionpool:https://newrepublic.com:443 \"GET / HTTP/1.1\" 200 41262\n",
      "DEBUG:root:https://newrepublic.com parsed\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): newrepublic.com\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): newrepublic.com\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): newrepublic.com\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): newrepublic.com\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): newrepublic.com\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): newrepublic.com\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): newrepublic.com\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): newrepublic.com\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): newrepublic.com\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): newrepublic.com\n",
      "DEBUG:urllib3.connectionpool:https://newrepublic.com:443 \"GET / HTTP/1.1\" 200 41262\n",
      "DEBUG:urllib3.connectionpool:https://newrepublic.com:443 \"GET /pages/people HTTP/1.1\" 200 12208\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): newrepublic.com\n",
      "DEBUG:urllib3.connectionpool:https://newrepublic.com:443 \"GET /authors/josephine-huetlin HTTP/1.1\" 200 25768\n",
      "DEBUG:urllib3.connectionpool:https://newrepublic.com:443 \"GET /tags/press-release HTTP/1.1\" 200 80033\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): newrepublic.com\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): newrepublic.com\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): newrepublic.com\n",
      "DEBUG:urllib3.connectionpool:https://newrepublic.com:443 \"GET /authors/lauren-oyler HTTP/1.1\" 200 24312\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): newrepublic.com\n",
      "DEBUG:urllib3.connectionpool:https://newrepublic.com:443 \"GET /authors/rachel-wetzler HTTP/1.1\" 200 25772\n",
      "DEBUG:urllib3.connectionpool:https://newrepublic.com:443 \"GET /tags/politics HTTP/1.1\" 200 133945\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): newrepublic.com\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): newrepublic.com\n",
      "DEBUG:urllib3.connectionpool:https://newrepublic.com:443 \"GET /pages/rss HTTP/1.1\" 301 42\n",
      "DEBUG:urllib3.connectionpool:https://newrepublic.com:443 \"GET /rss.xml HTTP/1.1\" 200 225213\n",
      "DEBUG:urllib3.connectionpool:https://newrepublic.com:443 \"GET /authors/jennifer-wilson-2 HTTP/1.1\" 200 17338\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): newrepublic.com\n",
      "DEBUG:urllib3.connectionpool:https://newrepublic.com:443 \"GET /authors/vegas-tenold HTTP/1.1\" 200 49796\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): newrepublic.com\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): newrepublic.com\n",
      "DEBUG:urllib3.connectionpool:https://newrepublic.com:443 \"GET /authors/anthony-elghossain HTTP/1.1\" 200 16957\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): newrepublic.com\n",
      "DEBUG:urllib3.connectionpool:https://newrepublic.com:443 \"GET /authors/emily-atkin HTTP/1.1\" 200 124724\n",
      "DEBUG:urllib3.connectionpool:https://newrepublic.com:443 \"GET /tags/books HTTP/1.1\" 200 185378\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): newrepublic.com\n",
      "DEBUG:urllib3.connectionpool:https://newrepublic.com:443 \"GET /authors/marin-cogan HTTP/1.1\" 200 116781\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): newrepublic.com\n",
      "DEBUG:urllib3.connectionpool:https://newrepublic.com:443 \"GET /authors/win-mccormack HTTP/1.1\" 200 105949\n",
      "DEBUG:urllib3.connectionpool:https://newrepublic.com:443 \"GET /authors/alex-shephard HTTP/1.1\" 200 101795\n",
      "DEBUG:urllib3.connectionpool:https://newrepublic.com:443 \"GET /authors/bryce-covert HTTP/1.1\" 200 115107\n",
      "DEBUG:urllib3.connectionpool:https://newrepublic.com:443 \"GET /tags/photography HTTP/1.1\" 200 146626\n",
      "DEBUG:urllib3.connectionpool:https://newrepublic.com:443 \"GET /authors/matt-ford HTTP/1.1\" 200 118832\n",
      "DEBUG:urllib3.connectionpool:https://newrepublic.com:443 \"GET /authors/josephine-livingstone HTTP/1.1\" 200 101279\n",
      "DEBUG:urllib3.connectionpool:https://newrepublic.com:443 \"GET /authors/sarah-jones HTTP/1.1\" 200 118177\n",
      "DEBUG:urllib3.connectionpool:https://newrepublic.com:443 \"GET /tags/climate-change HTTP/1.1\" 200 124768\n",
      "DEBUG:urllib3.connectionpool:https://newrepublic.com:443 \"GET /tags/culture HTTP/1.1\" 200 118946\n",
      "DEBUG:resources.newspaper.newspaper.source:We are extracting from 23 categories\n"
     ]
    }
   ],
   "source": [
    "url = SOURCES['left'][0]\n",
    "collection = 'LEFT'\n",
    "paper = scrape(url, db=db, collection=collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://newrepublic.com/tags/photography',\n",
       " 'https://newrepublic.com/authors/lauren-oyler',\n",
       " 'https://newrepublic.com',\n",
       " 'https://newrepublic.com/tags/books',\n",
       " 'https://newrepublic.com/pages/people',\n",
       " 'https://newrepublic.com/authors/josephine-huetlin',\n",
       " 'https://newrepublic.com/authors/win-mccormack',\n",
       " 'https://newrepublic.com/authors/alex-shephard',\n",
       " 'https://newrepublic.com/tags/press-release',\n",
       " 'https://newrepublic.com/authors/vegas-tenold',\n",
       " 'https://newrepublic.com/authors/rachel-wetzler',\n",
       " 'https://newrepublic.com/authors/matt-ford',\n",
       " 'https://newrepublic.com/authors/emily-atkin',\n",
       " 'https://newrepublic.com/authors/jennifer-wilson-2',\n",
       " 'https://newrepublic.com/tags/politics',\n",
       " 'https://newrepublic.com/pages/rss',\n",
       " 'https://newrepublic.com/authors/marin-cogan',\n",
       " 'https://newrepublic.com/authors/anthony-elghossain',\n",
       " 'https://newrepublic.com/authors/sarah-jones',\n",
       " 'https://newrepublic.com/authors/bryce-covert',\n",
       " 'https://newrepublic.com/authors/josephine-livingstone',\n",
       " 'https://newrepublic.com/tags/climate-change',\n",
       " 'https://newrepublic.com/tags/culture']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[c.url for c in paper.categories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:resources.newspaper.newspaper.source:We are extracting from 4 categories\n"
     ]
    }
   ],
   "source": [
    "paper.parse_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper.set_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper.categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "tempfile.gettempdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tldextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tags/politics'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/tags/politics'\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tags', 'politics']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_chunks = [x for x in path.split('/') if len(x) > 0]\n",
    "path_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtractResult(subdomain='', domain='', suffix='')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if len(path_chunks) == 1 and len(path_chunks[0]) < 14:\n",
    "    valid_categories.append(domain + path)\n",
    "else:\n",
    "    if self.config.verbose:\n",
    "        print(('elim category url %s for >1 path chunks '\n",
    "               'or size path chunks' % p_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper.categories.append(url+path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://newrepublic.com',\n",
       " 'https://newrepublic.com/minutes',\n",
       " 'https://newrepublic.com/latest',\n",
       " 'https://newrepublic.com/magazine']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = [c.url for c in paper.categories]\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://newrepublic.com',\n",
       " 'https://newrepublic.com/minutes',\n",
       " 'https://newrepublic.com/latest',\n",
       " 'https://newrepublic.com/magazine',\n",
       " 'https://newrepublic.com/tags/politics']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls.append(url+path)\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<resources.newspaper.newspaper.source.Category at 0x1053c2630>,\n",
       " <resources.newspaper.newspaper.source.Category at 0x1053c2390>,\n",
       " <resources.newspaper.newspaper.source.Category at 0x1053c22e8>,\n",
       " <resources.newspaper.newspaper.source.Category at 0x1053c2c50>,\n",
       " <resources.newspaper.newspaper.source.Category at 0x1053c20b8>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from resources.newspaper.newspaper.source import Category\n",
    "\n",
    "\n",
    "paper.categories = [Category(url=url) for url in urls]\n",
    "paper.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://newrepublic.com',\n",
       " 'https://newrepublic.com/minutes',\n",
       " 'https://newrepublic.com/latest',\n",
       " 'https://newrepublic.com/magazine',\n",
       " 'https://newrepublic.com/tags/politics']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = [c.url for c in paper.categories]\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = [\n",
    "            'about', 'help', 'privacy', 'legal', 'feedback', 'sitemap',\n",
    "            'profile', 'account', 'mobile', 'sitemap', 'facebook', 'myspace',\n",
    "            'twitter', 'linkedin', 'bebo', 'friendster', 'stumbleupon',\n",
    "            'youtube', 'vimeo', 'store', 'mail', 'preferences', 'maps',\n",
    "            'password', 'imgur', 'flickr', 'search', 'subscription', 'itunes',\n",
    "            'siteindex', 'events', 'stop', 'jobs', 'careers', 'newsletter',\n",
    "            'subscribe', 'academy', 'shopping', 'purchase', 'site-map',\n",
    "            'shop', 'donate', 'newsletter', 'product', 'advert', 'info',\n",
    "            'tickets', 'coupons', 'forum', 'board', 'archive', 'browse',\n",
    "            'howto', 'how to', 'faq', 'terms', 'charts', 'services',\n",
    "            'contact', 'plus', 'admin', 'login', 'signup', 'register',\n",
    "            'developer', 'proxy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'academy', 'account', 'admin', 'advert', 'archive', 'bebo', 'board', 'browse', 'careers', 'charts', 'contact', 'coupons', 'developer', 'donate', 'events', 'facebook', 'faq', 'feedback', 'flickr', 'forum', 'friendster', 'help', 'how to', 'howto', 'imgur', 'info', 'itunes', 'jobs', 'legal', 'linkedin', 'login', 'mail', 'maps', 'mobile', 'myspace', 'newsletter', 'newsletter', 'password', 'plus', 'preferences', 'privacy', 'product', 'profile', 'proxy', 'purchase', 'register', 'search', 'services', 'shop', 'shopping', 'signup', 'site-map', 'siteindex', 'sitemap', 'sitemap', 'stop', 'store', 'stumbleupon', 'subscribe', 'subscription', 'terms', 'tickets', 'twitter', 'vimeo', 'youtube']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(stops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
